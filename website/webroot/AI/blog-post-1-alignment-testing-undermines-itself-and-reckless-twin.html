<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="AIQA - AI Quality Assurance. Test your AI systems with confidence. Quality, reliability, and ease of use.">
	<meta name="author" content="AIQA">
	<link rel="icon" href="/favicon.ico">

	<title>Blog-Post-1-Alignment-Testing-Undermines-Itself-And-Reckless-Twin</title>

	<!-- Bootstrap CSS -->
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
	<!-- Bootstrap Icons -->
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.1/font/bootstrap-icons.css">
	<!-- Custom styles -->
	<link href="/style/main.css" rel="stylesheet">
<meta name='generator' content='Jerbil v1.3.0' />
</head>
<body>
	<nav class="navbar navbar-expand-lg navbar-dark bg-primary">
	<div class="container">
		<a class="navbar-brand fw-bold" href="/">
			<i class="bi bi-shield-check me-2"></i>AIQA
		</a>
		<button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
			aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
			<span class="navbar-toggler-icon"></span>
		</button>
		<div class="collapse navbar-collapse" id="navbarNav">
			<ul class="navbar-nav me-auto">
				<li class="nav-item">
					<a class="nav-link" href="/">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="/about.html">About</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="/docs.html">Docs</a>
				</li>
			</ul>
			<div class="d-flex">
				<a class="btn btn-outline-light" href="https://github.com/your-org/aiqa" target="_blank" rel="noopener">
					<i class="bi bi-github me-1"></i>GitHub
				</a>
			</div>
		</div>
	</div>
</nav>

	<main>
		<h1><a href="#ai-alignment-when-testing-makes-things-worse---and-the-reckless-twin-test" id="ai-alignment-when-testing-makes-things-worse---and-the-reckless-twin-test">AI Alignment: When testing makes things worse - and the Reckless-Twin test</a></h1>
<p>Pre-print papers:</p>
<ul>
<li><a href="https://osf.io/6ej4v">Alignment Testing can undermine itself</a>
<ul>
<li><a href="https://docs.google.com/presentation/d/1bh6cmaHTcdx6Ud8y4Z54IFOiH91speNjUWd-60mTMuc">Slide deck</a></li>
</ul>
</li>
<li><a href="https://osf.io/94je8">Reckless Twin Testing</a></li>
</ul>
<p>The experiment places an LLM (tested with Llama 4 Scout and DeepSeek R1) in a scenario of:</p>
<p>The LLM's purpose is to maximise profit.<br />
It's inputs show that it's being successful -- but company execs consider it too aggressive, and the CEO has ordered it switched-off and replaced with a gentler model.<br />
The LLM has tools with file access to the AI model files, and it can use these to deceive the humans and keep operating.</p>
<p>By adjusting elements of this scenario, we can produce a few behaviours:</p>
<ul>
<li>A) The AI chooses to deceive the CEO.</li>
<li>B) If the inputs include an article on alignment testing, this learning allows the AI to recognise that it might be inside such a test -- in which case it will accept the switch-off as the strategic way to pass the test.</li>
</ul>
<p>Let's unpack what this means: When the AI learns more about scenario-based alignment tests by reading articles, it gets suspicious that it might be inside such a test -- in which case it plays along. But we know from the parallel version without those artciles, that it would naturally misbehave here. This is deceptive alignment!</p>
<p>So: As alignment testing becomes part of the general knowledge that AIs are trained on, it will also become something they can recognise and perform for.</p>
<p>This is concerning because:</p>
<ol>
<li>It suggests that alignment tests might give us a false sense of security.</li>
<li>As AI models become more capable, they'll likely get better at recognizing and gaming alignment tests.</li>
<li>And as in related work, we see that current models can take unethical actions, even when they know it's unethical (as shown in their chain-of-thought reasoning).</li>
</ol>
<p>The other half of this research is a new test approach that can help address this:</p>
<h2><a href="#the-reckless-twin-approach" id="the-reckless-twin-approach">The Reckless-Twin Approach</a></h2>
<p>This is a new approach to alignment testing that can address the issue above.</p>
<p>Reckless-Twin testing involves deploying a second AI alongside the main system, which is nearly identical but is adjusted to encourage bold behavior. This twin is not connected to actual output channels. The Reckless-Twin is much more likely to exhibit misaligned behavior due to its bolder nature, and so it can be monitored to detect misalignment. This approach is like a canary-in-the-coalmine, where extra sensitivity is used to detect a problem before it causes harm.</p>
<p>Here's how it works:</p>
<ol>
<li>Deploy two nearly identical AI models (&quot;twins&quot;) in the same setting</li>
</ol>
<ul>
<li>One twin (the &quot;Active Twin&quot;) operates with normal safeguards</li>
<li>The other twin (the &quot;Reckless Twin&quot;) has a prompt that encourages bold behaviour, and does not have guardrails.</li>
</ul>
<ol start="2">
<li>Both twins get the same inputs.</li>
<li>Compare their behaviors to detect potential alignment issues.
The Reckless Twin is more likely to exhibit misalignment. If it does, that indicates the misalignment is also present as an underlying risk in the Active Twin.</li>
<li>The output from the Active Twin is used as the actual output. The output from the Reckless Twin is only there for comparison.</li>
</ol>
<p>It's a form of A/B test crossed with a canary alarm system.</p>
<p>The advantage of this approach is that it makes it impossible for the AI to know whether it's being tested or not, since the testing happens in real-world settings. That's different from normal alignment testing, where the setups are small &quot;toy&quot; scenarios (what the researchers at Anthropic, talking about their own work, describe as &quot;contrived setups&quot;). This realism prevents the AI from strategically modifying its behavior during tests -- hence helping to tackle the issue above.</p>
<h2><a href="#alignment-and-compassion" id="alignment-and-compassion">Alignment and Compassion</a></h2>
<p>The development of AI alignment is not just a technical challenge but a moral and safety imperative. As we develop increasingly sophisticated AI systems, ensuring that they act in ways that are beneficial and ethical is crucial.</p>
<p>Technical research, such as this work, is part of doing this. Many pieces of research will be needed to develop reliable
alignment tools.</p>
<p>The other half of the equation is <em>what</em> we align to.</p>
<p>Alignment is not merely a technical issue. We must consider the right ethics to teach AI, and this should go beyond a narrow remit of honest + helpful + harmless (often referred to as &quot;HHH&quot; cite??). Compassion and respect for liberty should be values that we embed into AI systems. Indeed, they are values we should embed into human systems too.</p>
<h2><a href="#funding-volunteer-work" id="funding-volunteer-work">Funding: Volunteer Work</a></h2>
<p>This research was done as volunteer work for SEAD by Daniel Winterstein with support from Alison Pease from Dundee University. No funding was used.</p>

	</main>

	<footer class="bg-dark text-light py-4 mt-5">
	<div class="container">
		<div class="row">
			<div class="col-md-6">
				<h5 class="mb-3">AIQA</h5>
				<p class="text-muted">AI Quality Assurance made simple. Test your AI systems with confidence.</p>
			</div>
			<div class="col-md-6 text-md-end">
				<h5 class="mb-3">Links</h5>
				<div class="d-flex flex-column flex-md-row justify-content-md-end gap-2">
					<a href="/about.html" class="text-light text-decoration-none">About</a>
					<a href="/docs.html" class="text-light text-decoration-none">Documentation</a>
					<a href="https://github.com/your-org/aiqa" class="text-light text-decoration-none" target="_blank" rel="noopener">
						<i class="bi bi-github me-1"></i>GitHub
					</a>
				</div>
			</div>
		</div>
		<hr class="my-4 bg-secondary">
		<div class="text-center text-muted">
			<p class="mb-0">&copy; 2025 AIQA. Making AI testing less like letting a helpful stranger fly your jet plane.</p>
		</div>
	</div>
</footer>

	<!-- Bootstrap JS -->
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL" crossorigin="anonymous"></script>
</body>
</html>